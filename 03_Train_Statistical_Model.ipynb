{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, r2_score, mean_squared_error\n",
    "from sklearn.linear_model import SGDClassifier, LinearRegression\n",
    "from sklearn import svm\n",
    "\n",
    "from textstat import textstat\n",
    "import stanza\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "import spacy\n",
    "import neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "f1_scorer = make_scorer(f1_score, pos_label=\"machine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assorted Setup Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializer tokenizers and lemmatizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza.download('en') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_lem = stanza.Pipeline(lang='en', processors='tokenize,lemma', use_gpu=False) # GPU issues in current stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency', use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_spacy = spacy.load('en')\n",
    "neuralcoref.add_to_pipe(nlp_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Lists for Complex Phrasal Features\n",
    "We obtained these by scraping several websites (see scraping code).  These take the form of special word lists that may occur less frequently in computer-generated text.  We've constructed three phrase datasets for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Load the lemmas if we've already generated them\n",
    "if (\n",
    "    Path(\"intermediate_data/cliche_lemmas.pkl\").is_file() and\n",
    "    Path(\"intermediate_data/archaisms_lemmas.pkl\").is_file() and\n",
    "    Path(\"intermediate_data/idiom_lemmas.pkl\").is_file()):\n",
    "    print(\"Loading generated lemmas...\")\n",
    "    \n",
    "    with open(\"intermediate_data/cliche_lemmas.pkl\", \"rb\") as f:\n",
    "        cliches = pickle.load(f)\n",
    "    with open(\"intermediate_data/archaisms_lemmas.pkl\", \"rb\") as f:\n",
    "        archaisms = pickle.load(f)\n",
    "    with open(\"intermediate_data/idiom_lemmas.pkl\", \"rb\") as f:\n",
    "        wiki_english_idioms = pickle.load(f)\n",
    "else:\n",
    "    print(\"Re-generating lemmas and saving them...\")\n",
    "    # Provided a copy here to make things easier: https://github.com/ecrows/cliche500\n",
    "    with open(\"./data/cliche500/cliches.txt\", \"r\") as f:\n",
    "        cliches = f.read().splitlines()\n",
    "\n",
    "    with open(\"./data/archaisms/archaisms.txt\", \"r\") as f:\n",
    "        archaisms = f.read().splitlines()\n",
    "\n",
    "    with open(\"./data/idioms/wiki_english_idioms.txt\", \"r\") as f:\n",
    "        wiki_english_idioms = f.read().splitlines()\n",
    "        # TODO: Drop any that contain colons?\n",
    "\n",
    "    print(cliches[:5])\n",
    "    print(archaisms[:5])\n",
    "    print(wiki_english_idioms[617:622])\n",
    "\n",
    "    # Convert all phrase features to lemmas.\n",
    "\n",
    "    for i,c in enumerate(cliches):\n",
    "        cliches[i] = [b.lemma for b in nlp_lem(c.lower()).iter_words()]\n",
    "\n",
    "    for i,c in enumerate(archaisms):\n",
    "        archaisms[i] = [b.lemma for b in nlp_lem(c.lower()).iter_words()]\n",
    "\n",
    "    for i,c in enumerate(wiki_english_idioms):\n",
    "        wiki_english_idioms[i] = [b.lemma for b in nlp_lem(c.lower()).iter_words()]\n",
    "\n",
    "    with open(\"intermediate_data/cliche_lemmas.pkl\", \"wb\") as f:\n",
    "        pickle.dump(cliches, f)\n",
    "    with open(\"intermediate_data/archaisms_lemmas.pkl\", \"wb\") as f:\n",
    "        pickle.dump(archaisms, f)\n",
    "    with open(\"intermediate_data/idiom_lemmas.pkl\", \"wb\") as f:\n",
    "        pickle.dump(wiki_english_idioms, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also harvest the Yorkshire dialect corpus and use that as well, but given that the feature was the weakest in the statistical paper, the dataset is not available in easily computer-readable form, and we don't expect yorkshire dialect to be particularly present in either class, we will omit this for now.\n",
    "If interested, features might be harvested from https://www.yorkshiredialect.com/words/A.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_355m_k40 = pd.read_json(\"./data/gpt-2-output-dataset/data/medium-345M-k40.train.jsonl\", lines=True)\n",
    "webtext = pd.read_json(\"./data/gpt-2-output-dataset/data/webtext.train.jsonl\", lines=True)\n",
    "gpt2_355m_k40_test = pd.read_json(\"./data/gpt-2-output-dataset/data/medium-345M-k40.test.jsonl\", lines=True)\n",
    "webtext_test = pd.read_json(\"./data/gpt-2-output-dataset/data/webtext.test.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_355m_k40['class'] = \"machine\"\n",
    "webtext['class'] = \"human\"\n",
    "gpt2_355m_k40_test['class'] = \"machine\"\n",
    "webtext_test['class'] = \"human\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = gpt2_355m_k40.append(webtext, ignore_index=True).sample(frac=1)\n",
    "df_train['text_lower'] = df_train['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_train = df_train.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_train = pd.read_csv(\"intermediate_data/3k_of_10k_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lemmas = subsample_train[\"text_lower\"].apply(lambda a: [b.lemma for b in nlp_lem(a).iter_words()])\n",
    "subsample_train[\"text_lower_lemmas\"] = text_lemmas\n",
    "subsample_train.to_csv(\"./intermediate_data/10k_subsample_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Frequency Features\n",
    "\n",
    "We get the distribution of lemmas within input documents, and take the log of rank and log of number of occurrences, and calculate a slope feature of a linear regression line for each document.  We then evaluate the quality of this linear regression line by comparing the lemma distribution to a Zipfian distribution.  We would expect this line to fit better if the distribution is more \"Zipfian\", as seen in human-generated text in past work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_freq_features(df):\n",
    "    col = df[\"text_lower_lemmas\"]\n",
    "    freq_features = []\n",
    "    \n",
    "    for b in col:\n",
    "        if type(b) is str:\n",
    "            print(\"Error: string type, expected list\")\n",
    "            raise TypeError\n",
    "            \n",
    "        tokens, counts = np.unique(b, return_counts=True)\n",
    "        \n",
    "        if len(tokens) == 0:\n",
    "            freq_features.append((0, 0, 0))\n",
    "            continue\n",
    "\n",
    "        log_counts = sorted(np.log(counts), reverse=True)\n",
    "        ranks = np.log(np.arange(1, len(log_counts)+1)).reshape(-1,1)\n",
    "        reg = LinearRegression().fit(ranks, log_counts)\n",
    "\n",
    "        preds = reg.predict(ranks)\n",
    "\n",
    "        r2 = r2_score(log_counts, preds)\n",
    "        slope = reg.coef_[0]\n",
    "        mse = mean_squared_error(log_counts, preds)\n",
    "\n",
    "        freq_features.append((slope, r2, mse))\n",
    "        \n",
    "    return freq_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_freq_features = generate_freq_features(subsample_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_freq_features, columns=[\"Slope\", \"R2\", \"MSE\"]).to_csv(\"features/frequency_features_train_10k.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Consistency Features\n",
    "\n",
    "We use a parsing tree to identify phrasal verbs, and calculate the ratio of phrasal verbs to the number of words.  We also calculate coreference resolution relationships that reflect text cohesion.  Greater numbers of coreference resolution per number of words indicate a higher likelihood that text is human-generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verb Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_verb_phrase_ratios(df):\n",
    "    col = df['text']\n",
    "    phrasal_ratios = []\n",
    "    for s in tqdm(col, desc=\"Generating Verb Phrase Ratios\"):\n",
    "        phrasal_count = 0\n",
    "        doc = const_nlp(s)\n",
    "\n",
    "        for sentence in doc.sentences:\n",
    "            c = sentence.constituency\n",
    "\n",
    "            for p in c.yield_preterminals(): # for newer stanza\n",
    "                if p.label == 'RP':\n",
    "                    phrasal_count += 1\n",
    "\n",
    "        if (doc.num_words == 0):\n",
    "            phrasal_ratios.append(0)\n",
    "        else:\n",
    "            phrasal_ratios.append(phrasal_count/doc.num_words)\n",
    "        \n",
    "        # to allow resuming\n",
    "        with open(\"intermediate_data/latest_ratios_const.pkl\", \"wb\") as f:\n",
    "            pickle.dump(phrasal_ratios, f)\n",
    "        \n",
    "    return phrasal_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_phrasal_ratios = generate_verb_phrase_ratios(subsample_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_train = subsample_train.head(3000)\n",
    "subsample_train.to_csv(\"intermediate_data/3k_of_10k_train.csv\")\n",
    "pd.DataFrame(phrasal_ratios).to_csv(\"features/verb_phrase_ratios_train_3048_of_10k.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coreference Resolution Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_coref_ratios(df):\n",
    "    coref_ratios = []\n",
    "    col = df['text']\n",
    "    for s in tqdm(col, desc=\"Generating Coreference Ratios\"):\n",
    "        doc = nlp_spacy(s)\n",
    "        coref_ratios.append(len(doc._.coref_clusters)/len(doc))\n",
    "    return coref_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_coref_ratios = generate_coref_ratios(subsample_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(coref_ratios).to_csv(\"features/coreference_ratios_train_3k_of_10k.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_train.to_csv(\"features/subsample_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Fluency Features\n",
    "\n",
    "Also create features built from Gunning-Fog index and Flesch reading ease tests, as in “Fake News Detection using LDA Topic Modelling and K-Nearest Neighbor” at CSoNET 2021.\n",
    "\n",
    "We leverage the pip package \"textstat\" for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fluency_features(df):\n",
    "    gunning_fog_scores = []\n",
    "    flesch_reading_ease_scores = []\n",
    "\n",
    "    col = df['text']\n",
    "    \n",
    "    for s in tqdm(col, desc=\"Generating Fluency Features\"):\n",
    "        gunning_fog_scores.append(textstat.gunning_fog(s))\n",
    "        flesch_reading_ease_scores.append(textstat.flesch_reading_ease(s))\n",
    "        \n",
    "    return gunning_fog_scores, flesch_reading_ease_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gunning_fog_scores, train_flesch_reading_ease_scores = generate_fluency_features(subsample_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gunning_fog_scores).to_csv(\"features/3k_of_10k_gunning_fog_scores_train.csv\", index=False)\n",
    "pd.DataFrame(flesch_reading_ease_scores).to_csv(\"features/3k_of_10k_flesch_reading_ease_scores_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Complex Phrasal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_match_count(an, search):\n",
    "    \"\"\"\n",
    "    Find matching sequences of elements in an array, fairly efficiently\n",
    "    \"\"\"\n",
    "    if len(search) == 0:\n",
    "        return 0\n",
    "        \n",
    "    try:\n",
    "        search_index = 0\n",
    "        found_count = 0\n",
    "        \n",
    "        while search_index < len(an):\n",
    "            first_word_index = an.index(search[0], search_index)\n",
    "            if (an[first_word_index:first_word_index+len(search)] == search):\n",
    "                found_count += 1\n",
    "\n",
    "            search_index = first_word_index+1\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    return found_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rate_of_wordlist(df, wordlist):\n",
    "    \"\"\"\n",
    "    Given a dataframe and a list of words (or phrases),\n",
    "    get the number of occurences of these in the relevant dataframe column,\n",
    "    divided by the length of the list\n",
    "    \"\"\"\n",
    "    ratios = []\n",
    "    for t in tqdm(df['text_lower_lemmas'], desc=\"Generating Complex Phrasal Features\"):\n",
    "        count = 0\n",
    "        for w in wordlist:\n",
    "            count += array_match_count(t, w)\n",
    "\n",
    "        if (len(t) > 0):\n",
    "            ratios.append(count/len(t))\n",
    "        else:\n",
    "            ratios.append(0)\n",
    "    \n",
    "    return ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures # multithreading for speed here\n",
    "\n",
    "def get_all_wordlist_ratios(passed_df):\n",
    "    df = passed_df\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_ratio_archaisms = executor.submit(get_rate_of_wordlist, df, archaisms)\n",
    "        future_ratio_idioms = executor.submit(get_rate_of_wordlist, df, wiki_english_idioms)\n",
    "        future_ratio_cliches = executor.submit(get_rate_of_wordlist, df, cliches)\n",
    "        \n",
    "        ratio_archaisms = future_ratio_archaisms.result()\n",
    "        ratio_idioms = future_ratio_idioms.result()\n",
    "        ratio_cliches = future_ratio_cliches.result()\n",
    "        \n",
    "        return(list(zip(ratio_archaisms, ratio_idioms, ratio_cliches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratios = get_all_wordlist_ratios(subsample_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wordlist_ratios = train_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_ratios, columns=[\"archaisms\", \"idioms\", \"cliches\"]).to_csv(\"features/3k_of_10k_phrasal_wordlist_ratios.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "for i in range(0,subsample_train.shape[0]):\n",
    "    vector = []\n",
    "    vector.extend(train_freq_features[i])\n",
    "    \n",
    "    vector.append(train_phrasal_ratios[i])\n",
    "    vector.append(train_coref_ratios[i])\n",
    "    \n",
    "    vector.extend(train_wordlist_ratios[i])\n",
    "    \n",
    "    vector.append(gunning_fog_scores[i])\n",
    "    vector.append(flesch_reading_ease_scores[i])\n",
    "\n",
    "    features.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(features, columns=[\"Slope\", \"R2\", \"MSE\", \"Verb Phrase\", \"Coreference\", \"Archaisms\", \"Idioms\", \"Cliches\", \"Gunning-Fog\", \"Flesch\"]).to_csv(\"features/3k_of_10k_combined_features.csv\", index=False)\n",
    "features = pd.read_csv(\"features/3k_of_10k_combined_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "x = scaler.transform(x) # Have to re-use this scaler later to prevent any data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=10000)\n",
    "clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_c(feats):\n",
    "    print(\"Hyperparameter search\")\n",
    "    param_grid = [\n",
    "      {'C': [1, 10, 100, 1000]},\n",
    "     ]\n",
    "\n",
    "    svc = svm.SVC(kernel='linear', random_state=0)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=0)\n",
    "    search = GridSearchCV(estimator=svc, param_grid=param_grid, scoring=f1_scorer, cv=cv, verbose=10)\n",
    "\n",
    "    _x = feats\n",
    "    _y = subsample_train['class']\n",
    "    \n",
    "    search.fit(_x, _y)\n",
    "    print(search.best_params_)\n",
    "    return search.best_params_['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This comes out to C=100\n",
    "best_stat_c = get_best_c(x) # 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = subsample_train['class']\n",
    "clf = svm.SVC(C=100, kernel='linear', probability=True, random_state=0)\n",
    "clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = clf.predict(x)\n",
    "print(f\"Train set accuracy: {accuracy_score(subsample_train['class'], train_results):.4f}\")\n",
    "print(f\"Train set F1 score: {f1_score(subsample_train['class'], train_results, pos_label='machine'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/linear_svm_3k_of_10k_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/linear_svm_3k_of_10k_proba.pickle', 'wb') as f:\n",
    "    pickle.dump(clf, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model Against GPT-355M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLES_OF_EACH = 500 # How many human and machine examples to put into test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = gpt2_355m_k40_test.sample(EXAMPLES_OF_EACH, random_state=0).append(webtext_test.sample(EXAMPLES_OF_EACH, random_state=0), ignore_index=True).sample(frac=1, random_state=0)\n",
    "df_test['text_lower'] = df_test['text'].str.lower()\n",
    "df_test['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_test[\"text_lower_lemmas\"] = subsample_test[\"text_lower\"].apply(lambda a: [b.lemma for b in nlp_lem(a).iter_words()])\n",
    "subsample_test.to_pickle(\"./intermediate_data/1k_subsample_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_test = pd.read_pickle(\"./intermediate_data/1k_subsample_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all features for test set\n",
    "test_freq_features = generate_freq_features(subsample_test)\n",
    "test_verb_phrase = generate_verb_phrase_ratios(subsample_test)\n",
    "test_coref_ratios = generate_coref_ratios(subsample_test)\n",
    "test_phrasal_ratios = get_all_wordlist_ratios(subsample_test)\n",
    "test_gf_scores, test_fre_scores = generate_fluency_features(subsample_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = []\n",
    "for i in range(0,subsample_test.shape[0]):\n",
    "    vector = []\n",
    "    vector.extend(test_freq_features[i])\n",
    "    \n",
    "    vector.append(test_verb_phrase[i])\n",
    "    vector.append(test_coref_ratios[i])\n",
    "    \n",
    "    vector.extend(test_phrasal_ratios[i])\n",
    "    \n",
    "    vector.append(test_gf_scores[i])\n",
    "    vector.append(test_fre_scores[i])\n",
    "\n",
    "    test_features.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\"Slope\", \"R2\", \"MSE\", \"Verb Phrase\", \"Coreference\", \"Archaisms\", \"Idioms\", \"Cliches\", \"Gunning-Fog\", \"Flesch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_features, columns=feature_columns).to_csv(\"features/1k_combined_features_test.csv\", index=False)\n",
    "test_features = pd.read_csv(\"features/1k_combined_features_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling using previous scaler from training\n",
    "x_test = test_features\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = clf.predict(x_test)\n",
    "print(f\"Test set accuracy: {accuracy_score(subsample_test['class'], test_results):.4f}\")\n",
    "print(f\"Test set F1 score: {f1_score(subsample_test['class'], test_results, pos_label='machine'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot relative feature importance of each feature for classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(abs(clf.coef_[0]), index=feature_columns).nlargest(10).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/svm-sgd-complex-phrase.pickle\", \"wb\") as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/svm-sgd-complex-phrase.pickle\", \"rb\") as f:\n",
    "    clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate against GPT-2 1.5B and GPT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_1542m_k40_test = pd.read_json(\"./data/gpt-2-output-dataset/data/xl-1542M-k40.test.jsonl\", lines=True)\n",
    "webtext_test = pd.read_json(\"./data/gpt-2-output-dataset/data/webtext.test.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3 = pd.read_json(\"./data/gpt3_175b_samples.jsonl\", lines=True)\n",
    "# Fixing the dataset since <|endoftext|> is included erroneously.  OpenAI should probably accept pull requests for this sort of thing...\n",
    "gpt3_rebuild = []\n",
    "for f in gpt3[0]:\n",
    "    split = f.split(\"<|endoftext|>\")\n",
    "    for s in split:\n",
    "        gpt3_rebuild.append(s)\n",
    "gpt3_df = pd.DataFrame(gpt3_rebuild, columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_1542m_k40_test['class'] = \"machine\"\n",
    "webtext_test['class'] = \"human\"\n",
    "gpt3_df['class'] = \"machine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_1532m_eval = gpt2_1542m_k40_test.sample(EXAMPLES_OF_EACH, random_state=0).append(webtext_test.sample(EXAMPLES_OF_EACH, random_state=0), ignore_index=True)\n",
    "gpt2_1532m_eval = gpt2_1532m_eval.sample(frac=1, random_state=0)\n",
    "gpt2_1532m_eval['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_1532m_eval['text_lower'] = gpt2_1532m_eval['text'].str.lower()\n",
    "gpt2_1532m_eval[\"text_lower_lemmas\"] = gpt2_1532m_eval[\"text_lower\"].apply(lambda a: [b.lemma for b in nlp_lem(a).iter_words()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_1532m_eval.to_pickle(\"./intermediate_data/1k_subsample_gpt2_1532m_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_1532m_eval = pd.read_pickle(\"./intermediate_data/1k_subsample_gpt2_1532m_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_eval = gpt3_df.append(webtext_test.sample(gpt3_df.shape[0]), ignore_index=True)\n",
    "gpt3_eval = gpt3_df.sample(EXAMPLES_OF_EACH, random_state=0).append(webtext_test.sample(EXAMPLES_OF_EACH, random_state=0), ignore_index=True)\n",
    "gpt3_eval = gpt3_eval.sample(frac=1, random_state=0)\n",
    "gpt3_eval['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_eval['text_lower'] = gpt3_eval['text'].str.lower()\n",
    "gpt3_eval[\"text_lower_lemmas\"] = gpt3_eval[\"text_lower\"].apply(lambda a: [b.lemma for b in nlp_lem(a).iter_words()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_eval.to_pickle(\"./intermediate_data/1k_subsample_gpt3_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_eval = pd.read_pickle(\"./intermediate_data/1k_subsample_gpt3_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_features(df, vp_feats=None):\n",
    "    if vp_feats is not None: # had to do this piecemeal on CPU to avoid stanza crashes\n",
    "        _verb_phrase = vp_feats\n",
    "    else:\n",
    "        _verb_phrase = generate_verb_phrase_ratios(df)\n",
    "    _freq_features = generate_freq_features(df)\n",
    "    _coref_ratios = generate_coref_ratios(df)\n",
    "    _phrasal_ratios = get_all_wordlist_ratios(df)\n",
    "    _gf_scores, _fre_scores = generate_fluency_features(df)\n",
    "\n",
    "    test_features = []\n",
    "    for i in range(0,subsample_test.shape[0]):\n",
    "        vector = []\n",
    "        vector.extend(_freq_features[i])\n",
    "\n",
    "        vector.append(_verb_phrase[i])\n",
    "        vector.append(_coref_ratios[i])\n",
    "\n",
    "        vector.extend(_phrasal_ratios[i])\n",
    "\n",
    "        vector.append(_gf_scores[i])\n",
    "        vector.append(_fre_scores[i])\n",
    "\n",
    "        test_features.append(vector)\n",
    "        \n",
    "    return test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intermediate_data/latest_ratios_const-to463.pkl\", \"rb\") as f:\n",
    "    gpt2_1532m_features_to_463 = pickle.load(f)\n",
    "    \n",
    "with open(\"intermediate_data/latest_ratios_const-463-1000.pkl\", \"rb\") as f:\n",
    "    gpt2_1532m_features_463_1000 = pickle.load(f)\n",
    "gpt2_1532m_features_to_463.extend(gpt2_1532m_features_463_1000)\n",
    "\n",
    "Combine and output GPT2_1532M features\n",
    "pd.DataFrame(gpt2_1532m_features_to_463).to_csv(\"intermediate_data/gpt2_1532m_test_vp_complete.csv\", index=False)\n",
    "gpt2_1532m_features_to_463 = pd.read_csv(\"intermediate_data/gpt2_1532m_test_vp_complete.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Constituency parsing seems to be allocating tensors on CPU sporadically, inhibiting performance\n",
    "gpt2_1532m_features = generate_all_features(gpt2_1532m_eval, vp_feats = gpt2_1532m_features_to_463[\"0\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gpt2_1532m_features, columns=feature_columns).to_csv(\"features/1k_combined_features_gpt2_1532m_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_1532m_features = pd.read_csv(\"features/1k_combined_features_gpt2_1532m_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling using previous scaler from training\n",
    "x_test_gpt2_1532m = gpt2_1532m_features\n",
    "x_test_gpt2_1532m = scaler.transform(x_test_gpt2_1532m)\n",
    "test_gpt2_1532m_results = clf.predict(x_test_gpt2_1532m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"GPT2-1.5B Test set accuracy: {accuracy_score(test_gpt2_1532m_results, gpt2_1532m_eval['class']):.4f}\")\n",
    "print(f\"GPT2-1.5B Test set F1 score: {f1_score(test_gpt2_1532m_results,  gpt2_1532m_eval['class'], pos_label='machine'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intermediate_data/latest_ratios_const-gpt3-298.pkl\", \"rb\") as f:\n",
    "    gpt3_vp_to_298 = pickle.load(f)\n",
    "    \n",
    "with open(\"intermediate_data/latest_ratios_const-gpt3-298to.pkl\", \"rb\") as f:\n",
    "    gpt3_vp_298_on = pickle.load(f)\n",
    "    \n",
    "with open(\"intermediate_data/latest_ratios_const-gpt3-fin.pkl\", \"rb\") as f:\n",
    "    gpt3_vp_fin = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_vp_to_298.extend(gpt3_vp_298_on)\n",
    "gpt3_vp_to_298.extend(gpt3_vp_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intermediate_data/latest_vp_ratios_const_gpt3_combined.pkl\", \"wb\") as f:\n",
    "    pickle.dump(gpt3_vp_to_298, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_features = generate_all_features(gpt3_eval, vp_feats=gpt3_vp_to_298)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gpt3_features, columns=feature_columns).to_csv(\"features/1k_combined_features_gpt3_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling using previous scaler from training\n",
    "x_test_gpt3 = gpt3_features\n",
    "x_test_gpt3 = scaler.transform(x_test_gpt3)\n",
    "test_gpt3_results = clf.predict(x_test_gpt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"GPT3 Test set accuracy: {accuracy_score(test_gpt3_results, gpt3_eval['class']):.4f}\")\n",
    "print(f\"GPT3 Test set F1 score: {f1_score(test_gpt3_results, gpt3_eval['class'], pos_label='machine'):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "ta2",
   "name": "common-cu110.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m87"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
